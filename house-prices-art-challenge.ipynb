{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Kaggle House Prices Competition \n\n# Cleyton Candeira\n\nReal estate sales in the United States 🏡🇺🇸 can be influenced by various factors, including economic bargaining, which often plays a significant role in consumers' buying decisions. Instead of solely focusing on the physical features of a property, such as the number of bedrooms and bathrooms, buyers may also consider economic factors like the price, interest rates (not relevant in this context 😊), and the expected property appreciation.\n\nDuring the real estate boom in the USA, primarily in the early 2000s, there was a substantial increase in demand for properties, resulting in a considerable rise in home prices 📈🏠. During this period, many people viewed property purchases as a profitable investment, and some even transitioned into becoming real estate agents, capitalizing on the expanding market 📊💰.\n\n*Informative Addition*\n\nHowever, this real estate bubble was unsustainable, and in 2008, the subprime crisis occurred. This crisis was triggered by the collapse of the high-risk mortgage market (known as subprime mortgages), leading to mass borrower defaults and widespread property devaluation 📉🏚️. The real estate market collapsed, resulting in a significant economic recession in the USA and worldwide 🌍📉.\n\nThis financial crisis of 2008 had a lasting impact on the real estate sector and the overall economy, serving as a reminder of the risks associated with the real estate market and uncontrolled speculation. Since then, there have been significant changes in financial and mortgage regulations to prevent similar crises in the future 🏦🔒.\n\n","metadata":{}},{"cell_type":"markdown","source":"* The challenge seeks to find the best price prediction model for properties in the city of Ames, Iowa.","metadata":{}},{"cell_type":"markdown","source":"**I also took some inspiration and used some techniques of other people, but I did not hard-copied, he said**","metadata":{}},{"cell_type":"markdown","source":"## Data Treatment","metadata":{}},{"cell_type":"code","source":"#Basic Packages \n\nimport sys\nimport scipy\n\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport numpy as np\n!pip install numpy\nimport seaborn as sns\n\n\nprint('Environment specification:\\n')\nprint('python', '%s.%s.%s' % sys.version_info[:3])\n\nfor mod in np, scipy, sns, pd:\n    print(mod.__name__, mod.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T22:59:54.707753Z","iopub.execute_input":"2023-09-29T22:59:54.708046Z","iopub.status.idle":"2023-09-29T23:00:04.859205Z","shell.execute_reply.started":"2023-09-29T22:59:54.708020Z","shell.execute_reply":"2023-09-29T23:00:04.858027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n\ndf = pd.concat([df_train, df_test], sort = True, ignore_index = True)\n\ndf.head(15)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:04.860638Z","iopub.execute_input":"2023-09-29T23:00:04.860977Z","iopub.status.idle":"2023-09-29T23:00:04.943367Z","shell.execute_reply.started":"2023-09-29T23:00:04.860947Z","shell.execute_reply":"2023-09-29T23:00:04.942376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's check data type\n\ndef count_data_types(df):\n    data_types = df.dtypes\n    count = data_types.value_counts()\n    return count\n\nresult = count_data_types(df)\n\n#Show results dtypes\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:04.944726Z","iopub.execute_input":"2023-09-29T23:00:04.945045Z","iopub.status.idle":"2023-09-29T23:00:04.952012Z","shell.execute_reply.started":"2023-09-29T23:00:04.945016Z","shell.execute_reply":"2023-09-29T23:00:04.951036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SalesPrice Treatment","metadata":{}},{"cell_type":"code","source":"# Use the .describe() method to generate descriptive statistics\ndescription = df_train['SalePrice'].describe()\n\n# Print the descriptive statistics\nprint(description)\n\n# Create a figure with two subplots: boxplot and histogram\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Boxplot\nsns.set(style=\"whitegrid\")\nsns.boxplot(x=df_train['SalePrice'], ax=axes[0])\naxes[0].set_title('SalePrice Distribution (Boxplot)', fontsize=16)\naxes[0].set_xlabel('SalePrice', fontsize=14)\naxes[0].tick_params(axis='x', labelsize=12)\naxes[0].tick_params(axis='y', labelsize=12)\n\n# Histogram\nsns.histplot(df_train['SalePrice'], ax=axes[1], kde=True, color='skyblue')\naxes[1].set_title('SalePrice Distribution (Histogram)', fontsize=16)\naxes[1].set_xlabel('SalePrice', fontsize=14)\naxes[1].tick_params(axis='x', labelsize=12)\naxes[1].tick_params(axis='y', labelsize=12)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:04.954667Z","iopub.execute_input":"2023-09-29T23:00:04.954991Z","iopub.status.idle":"2023-09-29T23:00:05.547935Z","shell.execute_reply.started":"2023-09-29T23:00:04.954962Z","shell.execute_reply":"2023-09-29T23:00:05.546874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As it exhibits a non-normal probability distribution, it's crucial to conduct variable transformation tests to assess the attainment of a more 'normalized' distribution** 📊\n\nWe will test three different transformations (square root, logarithm, and Box-Cox) on the 'SalePrice' variable, and then it will display a boxplot alongside a histogram for each of them. Additionally, it will perform a normality test (e.g., Shapiro-Wilk test) after each transformation and show the p-value. The transformation that results in a p-value closer to 1 will indicate a more normal distribution. 📊🔄📈\n","metadata":{}},{"cell_type":"code","source":"# Assuming 'df' is your DataFrame with the 'SalePrice' variable\n\n# Define a list of transformation names and corresponding functions\ntransformations = {\n    'Original': lambda x: x,\n    'Square Root': np.sqrt,\n    'Logarithm': np.log1p\n}\n\n# Prepare a color palette for plotting\ncolors = sns.color_palette('Set2', n_colors=len(transformations))\n\n# Create subplots for boxplots and histograms\nfig, axes = plt.subplots(nrows=len(transformations), ncols=2, figsize=(12, 8))\n\nfor i, (transformation_name, transformation_func) in enumerate(transformations.items()):\n    # Apply the transformation to 'SalePrice'\n    transformed_price = transformation_func(df_train['SalePrice'])\n    \n    # Plot the boxplot\n    sns.boxplot(x=transformed_price, ax=axes[i, 0], color=colors[i])\n    axes[i, 0].set_title(f'Boxplot - {transformation_name}')\n    \n    # Plot the histogram\n    sns.histplot(transformed_price, ax=axes[i, 1], kde=True, color=colors[i])\n    axes[i, 1].set_title(f'Histogram - {transformation_name}')\n    \n    # Perform a normality test (Shapiro-Wilk) and display the p-value\n    p_value = stats.shapiro(transformed_price)[1]\n    axes[i, 1].text(0.6, 0.8, f'p-value: {p_value:.4f}', transform=axes[i, 1].transAxes)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:05.549457Z","iopub.execute_input":"2023-09-29T23:00:05.549764Z","iopub.status.idle":"2023-09-29T23:00:07.420885Z","shell.execute_reply.started":"2023-09-29T23:00:05.549735Z","shell.execute_reply":"2023-09-29T23:00:07.420001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As you can observe, the most effective transformation is the logarithmic one. It successfully normalizes the probability distribution of SalePrice in the best possible manner**","metadata":{}},{"cell_type":"code","source":"df_train[\"SalePrice\"] = np.log1p(df[\"SalePrice\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:07.422168Z","iopub.execute_input":"2023-09-29T23:00:07.422714Z","iopub.status.idle":"2023-09-29T23:00:07.428421Z","shell.execute_reply.started":"2023-09-29T23:00:07.422684Z","shell.execute_reply":"2023-09-29T23:00:07.427379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check NaN values ","metadata":{}},{"cell_type":"code","source":"# Calculate the percentage of NaN values in each column\nnan_percentage = (df_train.isna().mean() * 100).sort_values(ascending=False)\n\n# Filter columns with NaN values\nnan_columns = nan_percentage[nan_percentage > 0]\n\n# Create a bar plot to visualize the NaN percentages for selected columns\nplt.figure(figsize=(12, 6))\nsns.barplot(x=nan_columns, y=nan_columns.index, palette=\"viridis\")\nplt.xlabel('% of NaN Values')\nplt.ylabel('Columns')\nplt.title('Percentage of NaN Values per Column')\nplt.show()\n\n# Display columns with NaN values and their percentages\nprint(\"Columns with NaN values and their percentages:\")\nprint(nan_columns)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:07.429726Z","iopub.execute_input":"2023-09-29T23:00:07.430699Z","iopub.status.idle":"2023-09-29T23:00:07.875345Z","shell.execute_reply.started":"2023-09-29T23:00:07.430657Z","shell.execute_reply":"2023-09-29T23:00:07.874275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There are many variables with missing values, what to do?! Well, we can start from the simplest to the most complex. Categorical variables can be predicted through a KNN classification model. Can it be tiring? Maybe, but we'll start with those variables where there are less than 20% missing values, which are:**\n- LotFrontage\n- GarageYrBlt\n- GarageCond\n- GarageType\n- GarageFinish\n- GarageQual\n...\n\n**However, we cannot accurately predict missing values without a comprehensive understanding of the relationships between all variables, including their weights, correlations, and other relevant factors. So we'll first have to do an EDA of the House Pricing database...**","metadata":{}},{"cell_type":"markdown","source":"### EDA 📊🔍","metadata":{}},{"cell_type":"code","source":"#Remove Id Col\nif 'Id' in df_train.columns:\n    df_train.drop(['Id'], axis=1, inplace=True)\n    \n# Check if these columns are categorical or numeric\nqualitative = [col for col in df_train.columns if df_train[col].dtype == 'object']\nquantitative = [col for col in df_train.columns if df_train[col].dtype != 'object']\n\n#Remove Id && SalePrice\nif 'SalePrice' in quantitative:\n    quantitative.remove('SalePrice')","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:07.876897Z","iopub.execute_input":"2023-09-29T23:00:07.877328Z","iopub.status.idle":"2023-09-29T23:00:07.889282Z","shell.execute_reply.started":"2023-09-29T23:00:07.877288Z","shell.execute_reply":"2023-09-29T23:00:07.888253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There are many qualitative and quantitative columns in the dataframe. We need to identify the most important columns that influence SalePrice before addressing missing values. This can save time and effort**","metadata":{}},{"cell_type":"markdown","source":"#### Qualitative EDA","metadata":{}},{"cell_type":"code","source":"# Fill NaN values in qualitative columns with \"Missing_Value\"\nfor col in qualitative:\n    df_train[col].fillna(\"Missing_Value\", inplace=True)\n\n# Create a loop to generate multiple boxplots\n\n# Number of boxplots per row\nboxplots_per_row = 2\n\n# Calculate the number of rows required\nnum_rows = (len(qualitative) + boxplots_per_row - 1) // boxplots_per_row\n\n# Loop to generate the boxplots\nfor i, col in enumerate(qualitative):\n    if i % boxplots_per_row == 0:\n        # Create a new figure at the beginning of each row\n        plt.figure(figsize=(12, 6))  # Adjust the figure size as needed\n\n    plt.subplot(1, boxplots_per_row, i % boxplots_per_row + 1)\n    sns.boxplot(x=col, y='SalePrice', data=df_train)\n    plt.xticks(rotation=90)\n    plt.title(f'Boxplot of SalePrice vs {col}')\n\n    if (i + 1) % boxplots_per_row == 0 or i == len(qualitative) - 1:\n        # If it's the last plot in the row, or a new row begins\n        plt.tight_layout()\n        plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:07.890535Z","iopub.execute_input":"2023-09-29T23:00:07.890806Z","iopub.status.idle":"2023-09-29T23:00:22.299628Z","shell.execute_reply.started":"2023-09-29T23:00:07.890782Z","shell.execute_reply":"2023-09-29T23:00:22.298551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a loop to generate multiple plots\nplot_rows = len(quantitative)\nplot_cols = 2  # Two plots per row\n\n# Create a list of warm colors\nwarm_colors = ['#FF5733', '#FFC300', '#FF5733', '#FFC300', '#FF5733', '#FFC300', '#FF5733']\n\n# Create a DataFrame to store normality test results\nnormality_results = pd.DataFrame(columns=['Variable', 'P-Value', 'Is Normal'])\n\n# Initialize the figure and axes outside the loop\nfig, axes = plt.subplots(nrows=plot_rows, ncols=plot_cols, figsize=(12, 4 * plot_rows))\n\n# Loop to generate the plots and perform normality tests\nfor i, col in enumerate(quantitative):\n    row = i // plot_cols\n    col_idx = i % plot_cols\n\n    # Use the warm color from the list\n    sns.histplot(df_train[col], kde=True, ax=axes[row, col_idx], color=warm_colors[i % len(warm_colors)])\n    axes[row, col_idx].set_title(f'Distribution of {col}')\n    axes[row, col_idx].set_xlabel(col)\n    axes[row, col_idx].set_ylabel('Frequency')\n\n    # Perform a normality test (Shapiro-Wilk)\n    _, p_value = stats.shapiro(df_train[col])\n    alpha = 0.05  # Significance level\n    is_normal = bool(p_value > alpha)  # Explicitly cast to bool dtype\n\n    # Add the results to the DataFrame\n    normality_results.loc[len(normality_results)] = [col, p_value, is_normal]\n\n# Remove empty subplots\nif len(quantitative) < plot_rows * plot_cols:\n    for i in range(len(quantitative), plot_rows * plot_cols):\n        fig.delaxes(axes.flatten()[i])\n\nplt.tight_layout()\nplt.show()\n\n# Print the normality test results DataFrame\nprint(normality_results)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:22.301034Z","iopub.execute_input":"2023-09-29T23:00:22.301888Z","iopub.status.idle":"2023-09-29T23:00:36.408442Z","shell.execute_reply.started":"2023-09-29T23:00:22.301826Z","shell.execute_reply":"2023-09-29T23:00:36.407657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Definition of ANOVA (Analysis of Variance)\n\nAn F-test is any statistical test in which the test statistic has an F-distribution under the null hypothesis. It is most often used when comparing statistical models that have been fitted to a data set, in order to identify the model that best fits the population from which the data were sampled. Exact \"F-tests\" mainly arise when the models have been fitted to the data using least squares. The name was coined by George W. Snedecor, in honour of Ronald Fisher. Fisher initially developed the statistic as the variance ratio in the 1920s. In summary, Analysis of Variance (ANOVA) is a statistical technique used to compare the means of three or more groups or categories. It is often used to test whether there are significant differences between the population means of distinct groups. It provides a structured approach to hypothesis testing and allows for deeper insights into group differences. However, it's crucial to use ANOVA appropriately, considering its underlying assumptions and conducting post-hoc tests when necessary, to draw valid conclusions from your data.\n\n### Mathematical Formulation\n\n\nLet's assume you have $(k)$ groups with $(n_i)$ observations in each group, where $(i = 1, 2, ..., k)$.\n\nThe statistical hypotheses are formulated as:\n\n- **Null Hypothesis $(H_0)$:** All population means are equal.\n  \n  $H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_k$\n\n- **Alternative Hypothesis $(H_1)$:** At least one population mean is different from the others.\n  \n  $H_1: \\text{At least one of the population means is different.}$\n\nThe F-test statistic is calculated as the ratio of between-group variability to within-group variability. The formula for the F-statistic is:\n\n$F = \\frac{\\text{Between-Group Variability}}{\\text{Within-Group Variability}}$\n\nWhere:\n\n- **Between-Group Variability** is the sum of squared differences between the means of each group and the overall mean, weighted by the number of observations in each group.\n  $\\text{Between-Group Variability} = \\sum_{i=1}^{k} n_i (\\bar{X}_i - \\bar{X})^2$\n  \n- **Within-Group Variability** is the sum of squared differences between each observation and the mean of the group to which it belongs.\n  \n  $\\text{Within-Group Variability} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X}_i)^2$\n\nHere, $(\\bar{X}_i)$ represents the mean of group $(i)$, and $(\\bar{X})$ is the overall mean.\n\nFinally, the F-statistic follows an F-distribution with $(k-1)$ and $(N - k)$ degrees of freedom, where $(N)$ is the total number of observations.\n\n\n**Reference:**\n- [ANOVA - Wikipedia](https://en.wikipedia.org/wiki/Analysis_of_variance)\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# Define a function to perform ANOVA\ndef perform_anova(frame):\n    # Create an empty DataFrame to store results\n    anova_results = pd.DataFrame()\n    anova_results['Feature'] = qualitative  # Assuming qualitative is predefined\n\n    p_values = []\n\n    # Iterate through qualitative features\n    for feature in qualitative:\n        # Create a list to store samples for each category\n        samples = []\n\n        # Iterate through unique categories within the feature\n        for category in frame[feature].unique():\n            # Get SalePrice values for the current category\n            sale_prices = frame[frame[feature] == category]['SalePrice'].values\n            samples.append(sale_prices)\n\n        # Perform ANOVA test and get the p-value\n        p_value = stats.f_oneway(*samples)[1]\n        p_values.append(p_value)\n\n    # Add p-values to the DataFrame\n    anova_results['p-value'] = p_values\n\n    # Sort the results by p-value\n    anova_results = anova_results.sort_values(by='p-value')\n\n    return anova_results\n\n# Perform ANOVA on the 'train' DataFrame\nresult = perform_anova(df_train)\n\n# Calculate the disparity for visualization\nresult['Disparity'] = np.log(1.0 / result['p-value'].values)\n\n# Create a barplot to visualize the results\nplt.figure(figsize=(12, 6))\nsns.barplot(data=result, x='Feature', y='Disparity')\nplt.xticks(rotation=90)\nplt.xlabel('Feature')\nplt.ylabel('Disparity (log(1/p-value))')\nplt.title('ANOVA Test Results')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:36.409645Z","iopub.execute_input":"2023-09-29T23:00:36.410327Z","iopub.status.idle":"2023-09-29T23:00:37.280958Z","shell.execute_reply.started":"2023-09-29T23:00:36.410296Z","shell.execute_reply":"2023-09-29T23:00:37.279864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort 'result' by the top 20 disparities\nresult_top_25 = result.sort_values(by='Disparity', ascending=False).head(25)\n\n# Get the names of variables from the top 20 results\ntop_25_features = result_top_25['Feature'].tolist()\n\n# Find the variables that match the names in 'top_20_features'\nmatching_variables = nan_columns[nan_columns.index.isin(top_25_features)]\n\ncols_to_fix = list(matching_variables.index)\n\nsns.set(style=\"whitegrid\")\n\n# Crie um gráfico de barras usando seaborn\nplt.figure(figsize=(12, 6))  # Ajuste o tamanho do gráfico conforme necessário\nsns.barplot(x=matching_variables.index, y=matching_variables.values, palette=\"viridis\")\n\n# Rotacione os rótulos do eixo x para facilitar a leitura\nplt.xticks(rotation=90)\n\n# Adicione rótulos e título\nplt.xlabel('Variáveis Correspondentes')\nplt.ylabel('Valores')\nplt.title('Variáveis Correspondentes entre \"nan_columns\" e Top 20 de \"result\"')\n\n# Ajuste automaticamente o layout para evitar cortar rótulos\nplt.tight_layout()\n\n# Exiba o gráfico\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:37.282050Z","iopub.execute_input":"2023-09-29T23:00:37.282355Z","iopub.status.idle":"2023-09-29T23:00:37.627941Z","shell.execute_reply.started":"2023-09-29T23:00:37.282326Z","shell.execute_reply":"2023-09-29T23:00:37.626914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Categorical Variables to Address Missing Values for SalePrice Prediction:**\n\n   - FireplaceQu\n   - GarageCond\n   - GarageType\n   - GarageFinish\n   - GarageQual\n   - BsmtExposure\n   - BsmtQual\n   - BsmtCond\n   - BsmtFinType1\n   - MasVnrType\n   - Electrical","metadata":{}},{"cell_type":"markdown","source":"## Quantitative","metadata":{}},{"cell_type":"markdown","source":"### Correlation (Spearman, Pearson, and Kendall)\n\nSpearman, Kendall, and Pearson are correlation coefficients used to measure the relationship between variables. Spearman is ideal for assessing monotonic relationships in ordinal data or when a non-linear relationship is suspected, as it focuses on rank orders. Kendall's Tau is also suitable for ordinal data and assesses the concordance of rankings, making it robust to outliers and non-linear relationships. Pearson is best suited for linear relationships between continuous variables, assuming a normal distribution. It's sensitive to outliers and may not capture non-linear associations effectively. The choice depends on the nature of the data and the type of relationship under investigation, with Spearman and Kendall being preferable when data is not normally distributed or non-linearity is expected.\n\n**Spearman Rank Correlation:**\n\n*Mathematical Definition:*\n\nThe Spearman rank correlation coefficient, denoted by $ρs$, is calculated as the correlation between the rankings (orders) of variables $X$ and $Y$. It is given by the formula:\n\n$\\rho_s = 1 - \\frac{6\\sum{d_i^2}}{n(n^2 - 1)}$\n\nWhere:\n- $d_i$ is the difference between the ranks of $X$ and $Y$ for the $i-th$ observation.\n- $n$ is the number of observations.\n\n**Pearson Correlation Coefficient:**\n\n*Mathematical Definition:* \n\nThe Pearson correlation coefficient, denoted by $\\rho$ (rho), measures the linear correlation between two continuous variables $XX$ and $YY$. It is given by the formula:\n\n   $\\rho = \\frac{\\sum{(X - \\bar{X})(Y - \\bar{Y})}}{\\sqrt{\\sum{(X - \\bar{X})^2} \\sum{(Y - \\bar{Y})^2}}}$\n\nWhere:\n- $\\bar{X}$ is the mean of $X$.\n- $\\bar{Y}$ is the mean of $Y$..\n\n**Kendall Correlation Coefficient (Kendall's Tau):**\n\n*Mathematical Definition:* \n\nThe Kendall correlation coefficient, denoted by $\\tau$ (tau), quantifies the agreement or disagreement in the rankings of pairs of observations between two variables. It is computed as:\n\n$\\tau = \\frac{{\\text{number of concordant pairs} - \\text{number of discordant pair}}}{{\\frac{1}{2}n(n-1)}}$\n\nWhere:\n- The number of concordant pairs represents pairs of observations with the same relative order in both variables.\n- The number of discordant pairs represents pairs of observations with differing relative orders in both variables.\n- n is the total number of observations.\n\nThese mathematical definitions elucidate the calculation process for each correlation coefficient, conveying the statistical relationship between variables $X$ and $Y$ in terms of rankings, linearity, or concordance, as applicable to each coefficient.\n\nThe chosen correlation is Spearman because it assumes a monotonic relationship between the independent variables and the SalePrice variable. This also means that their ranking order will be taken into account. ","metadata":{}},{"cell_type":"code","source":"# Calculating Spearman correlations between quantitative variables and SalePrice\ncorrelations = {}\nfor var in quantitative:\n    correlations[var] = df_train['SalePrice'].corr(df_train[var], method='spearman')\n\n# Creating a DataFrame to store correlations\ncorrelation_df = pd.DataFrame({'Variable': list(correlations.keys()), 'Spearman Correlation': list(correlations.values())})\n\n# Sorting correlations in descending order\ncorrelation_df = correlation_df.sort_values(by='Spearman Correlation', ascending=False)\n\n# Creating a bar plot of correlations with improved grid and custom colors\nplt.figure(figsize=(10, 12))\nax = sns.barplot(data=correlation_df, x='Spearman Correlation', y='Variable', palette='coolwarm_r')\nplt.title('Spearman Correlations with SalePrice')\nplt.xlabel('Spearman Correlation')\nplt.ylabel('Variable')\n\n# Adding a grid\nax.xaxis.grid(True, linestyle='--', alpha=0.6)\nax.set_axisbelow(True)\n\n# Adjusting the size of variable names\nplt.xticks(fontsize=12)  # You can adjust the font size as needed\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:37.633311Z","iopub.execute_input":"2023-09-29T23:00:37.633887Z","iopub.status.idle":"2023-09-29T23:00:38.252509Z","shell.execute_reply.started":"2023-09-29T23:00:37.633855Z","shell.execute_reply":"2023-09-29T23:00:38.251683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the literature ([reference link](https://www.jstor.org/stable/pdf/1631943.pdf)), it is observed that both positively and negatively monotonic values can be significantly correlated from -0.5, indicating a \"low\" correlation, to 0.5, indicating a \"high\" correlation. Therefore, let's identify the values that are more correlated and filter them based on the columns with NaN values.\n\n**Reference**\n\nWissler, C. (1905). The Spearman correlation formula. Science, 22(558), 309-311.","metadata":{}},{"cell_type":"code","source":"# Interval Spearman Corr\nsignificant_results = correlation_df[(correlation_df['Spearman Correlation'] > 0.5) | (correlation_df['Spearman Correlation'] < -0.5)]\n\n#Get the names\nsignificant_cols = significant_results['Variable'].tolist()\n\n# Find the variables that match the names in 'significant_cols'\nmatching_variables = nan_columns[nan_columns.index.isin(significant_cols)]\n\ncols_to_fix_quanti = list(matching_variables.index)\n\nprint(\"Among the missing values that are statistically significant for the research, the only found variable is 'Year garage was built':\")\nprint()\nvariables_to_fix = '\\n'.join(cols_to_fix_quanti)\nprint(f\"{variables_to_fix}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.253764Z","iopub.execute_input":"2023-09-29T23:00:38.254328Z","iopub.status.idle":"2023-09-29T23:00:38.261935Z","shell.execute_reply.started":"2023-09-29T23:00:38.254301Z","shell.execute_reply":"2023-09-29T23:00:38.260885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion**\n\nAccording to my favorite philosopher, Garfield, he says, \"Why do tomorrow what you can do a month from now?\" Are we obviously starting with the simplest and then moving on to the more difficult? First, I predict the numerical values, and then I classify the categorical variables? Does the order of factors alter the product? \n\nLet's consider that, if I develop a prediction model for my lone numerical variable, I need to take into account all the other variables in the DataFrame to check their relevance, just as I did for SalePrice. In this context, I can try to replace the categorical values, at least some of them (I promise I will try), starting with the basics, simply by analyzing their context and seeing how I can fill in the gaps depending on the extent of the missing values. In this case, I will use logic before harnessing the power of any Machine Learning model.","metadata":{}},{"cell_type":"markdown","source":"### Feature Enginer","metadata":{}},{"cell_type":"markdown","source":"Don't forget it!\n\n   - FireplaceQu\n   - GarageCond\n   - GarageType\n   - GarageFinish\n   - GarageQual\n   - BsmtExposure\n   - BsmtQual\n   - BsmtCond\n   - BsmtFinType1\n   - MasVnrType\n   - Electrical","metadata":{}},{"cell_type":"markdown","source":"**Electrical Sys**\n\nSeveral factors influence the electrical system of a residence. However, these \"endogenous\" factors, internal ones like wiring, grounding, etc., we do not have as information. However, we do know that the type of electrical system is directly related to the consumption pattern. For example, the use of air conditioning, housing zone, whether it's for agriculture, commerce, urban areas, type of housing, etc. Understanding electrical systems:\n\nElectrical System (Electrical):\n\n   - Standard Circuit Breakers & Romex (SBrkr): This is a modern and standard electrical system with circuit breakers and Romex wiring. It is suitable for most single-family homes and conventional residences.\n\n   - Fuse Box over 60 AMP and all Romex wiring (FuseA): A fuse box system with Romex wiring, typically suitable for older homes with higher electrical capacity.\n\n   - 60 AMP Fuse Box and mostly Romex wiring (FuseF): A fuse box system with limited electrical capacity, suitable for older homes with lower electrical demand.\n\n   - 60 AMP Fuse Box and mostly knob & tube wiring (FuseP): This electrical system is older and may be less safe. It is important to assess and update these systems to meet modern standards.\n\n   - Mixed (Mix): A mixed electrical system may indicate that different parts of the residence have different types of wiring and electrical capacity. It may be necessary to assess and standardize the electrical system.","metadata":{}},{"cell_type":"code","source":"df_train['Electrical'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.263054Z","iopub.execute_input":"2023-09-29T23:00:38.263318Z","iopub.status.idle":"2023-09-29T23:00:38.278988Z","shell.execute_reply.started":"2023-09-29T23:00:38.263295Z","shell.execute_reply":"2023-09-29T23:00:38.278312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I selected some features to look: \n    \n   - General conditions of the house (we assume that this characteristic is an implicit attribute of the electrical system) \n   - Use of air conditioning\n   - Miscellaneous features (we understand that if a house includes an elevator or has two garages, it likely indicates a robust electrical system, or am I entirely mistaken?), and...\n   - The geographical location (in commercial and industrial areas, the likelihood of an outdated electrical system might be lower due to safety concerns—unless it's Tyler Durden's house ???).\n   - Do you receive all the sanitation and energy services?\n   - Foundation -- Wood?","metadata":{}},{"cell_type":"code","source":"miss_electrical = df_train[df_train['Electrical'] == 'Missing_Value']\nmiss_electrical[['OverallCond', 'CentralAir', 'MiscFeature', 'MSZoning', 'Utilities', 'Foundation']]","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.280080Z","iopub.execute_input":"2023-09-29T23:00:38.280888Z","iopub.status.idle":"2023-09-29T23:00:38.295419Z","shell.execute_reply.started":"2023-09-29T23:00:38.280853Z","shell.execute_reply":"2023-09-29T23:00:38.294537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can preemptively deduce that the electrical system is within the database. Despite its moderate rating, the property is situated in a sparsely populated urban area, features air conditioning, and is serviced by all public utilities, including sanitation and electricity.","metadata":{}},{"cell_type":"code","source":"df_train['Electrical'] = df_train['Electrical'].replace('Missing_Value', df_train['Electrical'].mode()[0])","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.296574Z","iopub.execute_input":"2023-09-29T23:00:38.296881Z","iopub.status.idle":"2023-09-29T23:00:38.303781Z","shell.execute_reply.started":"2023-09-29T23:00:38.296827Z","shell.execute_reply":"2023-09-29T23:00:38.302886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MasVnrType**\n\nMasVnrType: Masonry veneer type\n\n  - BrkCmn\tBrick Common\n  - BrkFace\tBrick Face\n  - CBlock\tCinder Block\n  - None\tNone\n  - Stone\tStone\n       \nThe \"MasVnrType\" (Masonry Veneer Type) variable can be influenced by several factors, including:\n   - Overall House Condition (OverallQual): The overall condition and quality of the house may affect the choice of masonry veneer type. A higher-quality house may be more likely to have premium veneer options like stone.\n   - House Style (HouseStyle): Different house styles may be associated with specific masonry veneer types. For example, a traditional or colonial-style house may be more likely to feature brick veneer.\n   - Neighborhood (Neighborhood): The neighborhood in which the house is located can influence the choice of masonry veneer type. Homes in upscale neighborhoods may opt for stone veneer for a more luxurious appearance.\n   - Year Built (YearBuilt): Older homes may have different masonry veneer types compared to newer homes. Historic homes might feature traditional brick veneer.\n   - Exterior Quality (ExterQual): The quality of the exterior materials may determine the masonry veneer type. A house with a high-quality exterior may feature premium veneer options.\n   - Lot Size (LotArea): Larger lots may allow for more flexibility in choosing masonry veneer types. Stone veneer, for example, might be more common in houses with spacious lots.\n   - Architectural Style (BldgType): Different types of dwellings may have preferences for specific masonry veneer types. For example, single-family homes (1Fam) and townhouses (TwnhsE, TwnhsI) may have varying veneer choices.\n","metadata":{}},{"cell_type":"code","source":"df_train['MasVnrType'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.304891Z","iopub.execute_input":"2023-09-29T23:00:38.305672Z","iopub.status.idle":"2023-09-29T23:00:38.317342Z","shell.execute_reply.started":"2023-09-29T23:00:38.305632Z","shell.execute_reply":"2023-09-29T23:00:38.316636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"miss_masvnrt = df_train[df_train['MasVnrType'] == 'Missing_Value']\nmiss_masvnrt[['OverallQual','HouseStyle','Neighborhood', 'YearBuilt', 'ExterQual', 'LotArea', 'BldgType']]","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.318418Z","iopub.execute_input":"2023-09-29T23:00:38.319148Z","iopub.status.idle":"2023-09-29T23:00:38.345599Z","shell.execute_reply.started":"2023-09-29T23:00:38.319118Z","shell.execute_reply":"2023-09-29T23:00:38.344889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_year_a = df_train.loc[df_train['YearBuilt'] < 1960, ['YearBuilt', 'MasVnrType']]\nresult_year_b = df_train.loc[(df_train['YearBuilt'] < 1960), ['YearBuilt', 'MasVnrType', 'Neighborhood']]\nresult_year_b = result_year_b[result_year_b['Neighborhood'] == 'Crawfor']\n\nprint(\"Value Counts for MasVnrType in result_year_a:\")\nprint(result_year_a['MasVnrType'].value_counts())\n\nprint(\"\\nValue Counts for MasVnrType in result_year_b:\")\nprint(result_year_b['MasVnrType'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.346545Z","iopub.execute_input":"2023-09-29T23:00:38.347391Z","iopub.status.idle":"2023-09-29T23:00:38.360366Z","shell.execute_reply.started":"2023-09-29T23:00:38.347363Z","shell.execute_reply":"2023-09-29T23:00:38.359431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the houses located in old Crawfor (below 1960) have no cladding (81.1%) on them. Therefore, the likelihood of our simple house not having any is high. We'll use fashion to replace it. This is also justified by the quality of the house (6)","metadata":{}},{"cell_type":"code","source":"df_train.at[528, 'MasVnrType'] = 'None'","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.361593Z","iopub.execute_input":"2023-09-29T23:00:38.362613Z","iopub.status.idle":"2023-09-29T23:00:38.367351Z","shell.execute_reply.started":"2023-09-29T23:00:38.362583Z","shell.execute_reply":"2023-09-29T23:00:38.366486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The houses in Somerst were built in 2006-2007, have a quality rating of 7, good exterior compartment quality, and an average lot size ranging from 8k to 11k square feet, for the 1Fam (single-family residence) type, except for the residence with an area of 4k square feet, which is a Townhouse End Unit. For someone like me who knows nothing about North American architecture, I researched and understood that a Townhouse End Unit is a house that is part of a set of houses usually arranged in rows or connected groups. Each townhouse unit is a separate house but shares side walls with neighboring units. An \"End Unit\" typically offers more privacy and often has additional windows on one side, which can provide more natural light.","metadata":{}},{"cell_type":"code","source":"result_somerst = df_train.loc[(df_train['Neighborhood'] == 'Somerst' ), ['BldgType', 'MasVnrType', 'ExterQual', 'Neighborhood', 'SalePrice']]\nresult_somerst = result_somerst[(result_somerst['ExterQual'] == 'Gd') & (result_somerst['BldgType'] == '1Fam')]\nprint(\"Value Counts for MasVnrType in result_somerst:\")\nprint(result_somerst['MasVnrType'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.368662Z","iopub.execute_input":"2023-09-29T23:00:38.368962Z","iopub.status.idle":"2023-09-29T23:00:38.384412Z","shell.execute_reply.started":"2023-09-29T23:00:38.368937Z","shell.execute_reply":"2023-09-29T23:00:38.383337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We know that stone houses are more expensive, so to find out if our houses are made of stone, we just have to look at the average selling price of these houses. It will be an excellent tie-breaker for these odds. ","metadata":{}},{"cell_type":"code","source":"result_somerst.groupby('MasVnrType')['SalePrice'].agg(['count', 'mean', 'var']).sort_values(by='mean', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.385549Z","iopub.execute_input":"2023-09-29T23:00:38.386223Z","iopub.status.idle":"2023-09-29T23:00:38.404450Z","shell.execute_reply.started":"2023-09-29T23:00:38.386194Z","shell.execute_reply":"2023-09-29T23:00:38.403495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_somerst = df_train.loc[(df_train['Neighborhood'] == 'Somerst' ), ['BldgType', 'MasVnrType', 'ExterQual', 'Neighborhood', 'SalePrice']]\nresult_somerst = result_somerst[(result_somerst['ExterQual'] == 'Gd') & (result_somerst['BldgType'] == 'TwnhsE')]\nprint(\"Value Counts for MasVnrType in result_somerst:\")\nprint(result_somerst['MasVnrType'].value_counts())\n\nresult_somerst.groupby('MasVnrType')['SalePrice'].agg(['count', 'mean', 'var']).sort_values(by='mean', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.405772Z","iopub.execute_input":"2023-09-29T23:00:38.406265Z","iopub.status.idle":"2023-09-29T23:00:38.424338Z","shell.execute_reply.started":"2023-09-29T23:00:38.406229Z","shell.execute_reply":"2023-09-29T23:00:38.423283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can conclude that the single-family homes have a brick cladding, while the townhouse has no cladding. The latter is in a row, shares walls and is the last in its row to share a wall. It is probably not made of stone and has no cladding. ","metadata":{}},{"cell_type":"code","source":"df_train.at[977, 'MasVnrType'] = 'None'\ndf_train.at[973, 'MasVnrType'] = 'BrkFace'\ndf_train.at[650, 'MasVnrType'] = 'BrkFace'","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.425740Z","iopub.execute_input":"2023-09-29T23:00:38.426447Z","iopub.status.idle":"2023-09-29T23:00:38.430676Z","shell.execute_reply.started":"2023-09-29T23:00:38.426418Z","shell.execute_reply":"2023-09-29T23:00:38.429886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm pretty sure that the house in NridgHt has a stone cladding. It's rated 10 and it's a single-family house, so why not? We'll have to check it out, though. ","metadata":{}},{"cell_type":"code","source":"filtered_nrid = df_train[(df_train['Neighborhood'] == 'NridgHt') & (df_train['OverallQual'] == 10)]\nresul_nridt = filtered_nrid.groupby(['OverallQual', 'MasVnrType'])['SalePrice'].agg(['mean', 'var']).reset_index()\nresult_nridt = filtered_nrid['MasVnrType'].mode().reset_index(name='Mode')\nprint(resul_nridt)\nprint()\nprint(result_nridt)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.431676Z","iopub.execute_input":"2023-09-29T23:00:38.432114Z","iopub.status.idle":"2023-09-29T23:00:38.452778Z","shell.execute_reply.started":"2023-09-29T23:00:38.432088Z","shell.execute_reply":"2023-09-29T23:00:38.451586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.at[1243, 'MasVnrType'] = 'Stone'","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.454052Z","iopub.execute_input":"2023-09-29T23:00:38.454492Z","iopub.status.idle":"2023-09-29T23:00:38.459392Z","shell.execute_reply.started":"2023-09-29T23:00:38.454464Z","shell.execute_reply":"2023-09-29T23:00:38.458726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have three missin values left. Now all we have to do is replace them with the mode of each neighborhood, since the characteristics are only differentiated by the neighborhood itself, and there won't be any problems. ","metadata":{}},{"cell_type":"code","source":"#Calculate the mode of 'MasVnrType' for each neighborhood\nmode_by_neighborhood = df_train.groupby('Neighborhood')['MasVnrType'].agg(lambda x: x.mode().iloc[0])\n\n# Replace 'Missing_Value' values with the mode corresponding to the neighborhood\ndf_train['MasVnrType'] = df_train.apply(lambda row: mode_by_neighborhood[row['Neighborhood']] if row['MasVnrType'] == 'Missing_Value' else row['MasVnrType'], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.460251Z","iopub.execute_input":"2023-09-29T23:00:38.460822Z","iopub.status.idle":"2023-09-29T23:00:38.494814Z","shell.execute_reply.started":"2023-09-29T23:00:38.460795Z","shell.execute_reply":"2023-09-29T23:00:38.493765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's notice that starting from the variable 'BsmtFinType1', the missing values grow significantly, making it unfeasible to correct them interpretatively. Therefore, a set of specific statistical procedures will be necessary to correct these classification values.","metadata":{}},{"cell_type":"markdown","source":"### DecisionTreeClassifier to fill NaN values","metadata":{}},{"cell_type":"markdown","source":"**I will estimate this model to fill in both the NaN values of the qualitative and quantitative variables**\n\nTo identify correlated variables among the features with missing values that need to be imputed, we can leverage their qualitative nature by ranking them based on Kendall's correlation, as described earlier.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import kendalltau\n\ndef find_highly_correlated_variables(df_train, variables_of_interest, threshold=0.7):\n    correlated_variables_dict = {}\n    \n    for var in variables_of_interest:\n        correlations = []\n        for column in df_train.columns:\n            if column != var:\n                correlation, _ = kendalltau(df_train[var], df_train[column])\n                correlations.append((column, correlation))\n\n        highly_correlated = [(col, corr) for col, corr in correlations if abs(corr) >= threshold]\n\n        highly_correlated.sort(key=lambda x: abs(x[1]), reverse=True)\n\n        correlated_variables_dict[var] = highly_correlated\n\n    result_df = pd.DataFrame(columns=['Target', 'Feature', 'Kendall Cor'])\n\n    for var, correlated_vars in correlated_variables_dict.items():\n        df = pd.DataFrame({'Target': var, 'Feature': [col for col, _ in correlated_vars], 'Kendall Cor': [corr for _, corr in correlated_vars]})\n        result_df = pd.concat([result_df, df], ignore_index=True)\n\n    return result_df","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.496807Z","iopub.execute_input":"2023-09-29T23:00:38.497575Z","iopub.status.idle":"2023-09-29T23:00:38.506362Z","shell.execute_reply.started":"2023-09-29T23:00:38.497537Z","shell.execute_reply":"2023-09-29T23:00:38.505711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"variables_of_interest = ['FireplaceQu', \n                         'GarageCond', \n                         'GarageType', \n                         'GarageFinish', \n                         'GarageQual', \n                         'BsmtExposure', \n                         'BsmtQual', \n                         'BsmtCond', \n                         'BsmtFinType1']\n\ncorrelation_results = find_highly_correlated_variables(df_train, variables_of_interest, threshold=0.35)\n\n# Display the results table\nprint(correlation_results)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:38.508183Z","iopub.execute_input":"2023-09-29T23:00:38.508574Z","iopub.status.idle":"2023-09-29T23:00:39.680589Z","shell.execute_reply.started":"2023-09-29T23:00:38.508540Z","shell.execute_reply":"2023-09-29T23:00:39.679553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Where to start? The missing correlations are specifically associated with variables that exhibit high rates of missing values, such as FireplaceQu and BsmtExposure (the latter seemingly demonstrating an indifference or lack of association with other variables). ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom sklearn.compose import ColumnTransformer\n\ndef train_and_impute(df_train, correlation_results, qualitative, quantitative, mode=\"classification\"):\n    # Check unique values in the \"Target\" column\n    unique_targets = correlation_results['Target'].unique()\n\n    for target in unique_targets:\n        # Identify rows with missing values in the target column\n        missing_rows = df_train[df_train[target] == 'Missing_Value']\n        non_missing_rows = df_train[df_train[target] != 'Missing_Value']\n\n        # Filter the original DataFrame based on the current target value\n        features = correlation_results[correlation_results['Target'] == target].drop(columns=['Target', 'Kendall Cor'])\n        features = features['Feature'].to_list()\n\n        # Separate the features (X) and target labels (y) for non-missing values\n        X = non_missing_rows[features]\n        y = non_missing_rows[target]\n\n        # Combine the missing rows with non-missing rows for encoding\n        combined_rows = pd.concat([X, missing_rows[features]])\n\n        # Split Quali and Quanti to transform\n        quali = [var for var in features if var in qualitative]\n        quanti = [var for var in features if var in quantitative]\n\n        # Apply OneHotEncoding\n        if len(quali) > 0:\n            ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), quali)], remainder='passthrough')\n            X_encoded = ct.fit_transform(combined_rows[quali])\n            X_encoded = X_encoded.toarray()\n        else:\n            pass\n\n        if len(quanti) > 0:\n            ss = StandardScaler()\n            X_quanti_scaled = ss.fit_transform(combined_rows[quanti])\n            X_encoded = np.hstack((X_encoded, X_quanti_scaled))\n        else:\n            X_encoded = X_encoded\n\n        # Label Encoder to target for non-missing values\n        label_encoder = LabelEncoder()\n        y_encoded = label_encoder.fit_transform(y)\n\n        # Split into train (80%) and test (20%) set for non-missing values\n        X_train, X_test, y_train, y_test = train_test_split(X_encoded[:len(y_encoded)], y_encoded, test_size=0.2, random_state=42)\n\n        # Create a Decision Tree Classifier or Regressor based on the mode\n        if mode == \"classification\":\n            model = DecisionTreeClassifier(random_state=42)\n        elif mode == \"regression\":\n            model = DecisionTreeRegressor(random_state=42)\n        else:\n            raise ValueError(\"Mode must be 'classification' or 'regression'.\")\n\n        # Train the model\n        model.fit(X_train, y_train)\n\n        # Predict for rows with 'Missing_Value' in the target column\n        X_missing_encoded = X_encoded[len(y_encoded):]\n        if mode == \"classification\":\n            y_pred_missing = model.predict(X_missing_encoded)\n        elif mode == \"regression\":\n            y_pred_missing = model.predict(X_missing_encoded).astype(int)\n\n        # Update the original DataFrame with predictions\n        df_train.loc[df_train[target] == 'Missing_Value', target] = label_encoder.inverse_transform(y_pred_missing)\n\n        # Calculate accuracy or mean squared error based on the mode\n        if mode == \"classification\":\n            y_pred_test = model.predict(X_test)\n            accuracy_test = accuracy_score(y_test, y_pred_test)\n            print(f'Model accuracy on test data for {target}: {accuracy_test:.2f}')\n        elif mode == \"regression\":\n            y_pred_test = model.predict(X_test)\n            mse_test = mean_squared_error(y_test, y_pred_test)\n            print(f'Mean Squared Error on test data for {target}: {mse_test:.2f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:39.682192Z","iopub.execute_input":"2023-09-29T23:00:39.683013Z","iopub.status.idle":"2023-09-29T23:00:39.801904Z","shell.execute_reply.started":"2023-09-29T23:00:39.682973Z","shell.execute_reply":"2023-09-29T23:00:39.800747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_and_impute(df_train, correlation_results, qualitative, quantitative, mode=\"classification\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:39.803243Z","iopub.execute_input":"2023-09-29T23:00:39.803693Z","iopub.status.idle":"2023-09-29T23:00:39.923657Z","shell.execute_reply.started":"2023-09-29T23:00:39.803660Z","shell.execute_reply":"2023-09-29T23:00:39.922632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_with_missing_values = df_train.columns[df_train.eq('Missing_Value').any()]\nprint(\"Colunas com valores 'Missing_Value':\")\nprint(columns_with_missing_values)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:39.924858Z","iopub.execute_input":"2023-09-29T23:00:39.925541Z","iopub.status.idle":"2023-09-29T23:00:39.935969Z","shell.execute_reply.started":"2023-09-29T23:00:39.925513Z","shell.execute_reply":"2023-09-29T23:00:39.935179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Numerical col 'GarageYrBlt'","metadata":{}},{"cell_type":"code","source":"#Split missing from non missing in GYB\nnon_missing_gyb = df_train[df_train['GarageYrBlt'].isnull() == False]\n\n#Check correlation \nresult = find_highly_correlated_variables(non_missing_gyb, ['GarageYrBlt'], threshold = 0.4)\n\nfor col in quantitative:\n    df_train[col].fillna(\"Missing_Value\", inplace=True) #This Missing_Value marker for the numeric variables helps the regression function to identify what is non-missing from what is missing\n    \ntrain_and_impute(df_train, result, qualitative, quantitative, mode=\"regression\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T23:00:39.937226Z","iopub.execute_input":"2023-09-29T23:00:39.938236Z","iopub.status.idle":"2023-09-29T23:00:40.066867Z","shell.execute_reply.started":"2023-09-29T23:00:39.938204Z","shell.execute_reply":"2023-09-29T23:00:40.065871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}